{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"Ross: Uh, Rach, we're running low on resumes over here.\n",
    "\n",
    "            Monica: Do you really want a job with Popular Mechanics?\n",
    "\n",
    "            Chandler: Well, if you're gonna work for mechanics, those are the ones to work for.\n",
    "\n",
    "            Rachel: Hey, look, you guys, I'm going for anything here, OK? I cannot be a waitress anymore, I mean it. I'm sick of the lousy tips, I'm sick of being called Excuse me.\n",
    "            \n",
    "            Ross: Rach, did you proofread these?\n",
    "\n",
    "            Rachel: Uh... yeah, why?\n",
    "\n",
    "            Ross: Uh, nothing, I'm sure they'll be impressed with your excellent compuper skills.\n",
    "\n",
    "            Rachel: (upset) Oh my Goood! Oh, do you think it's on all of them?\n",
    "\n",
    "            Joey: Oh no, I'm sure the Xerox machine caught a few.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Ross: Uh, Rach, we're running low on resumes over here.\", 'Monica: Do you really want a job with Popular Mechanics?', \"Chandler: Well, if you're gonna work for mechanics, those are the ones to work for.\", \"Rachel: Hey, look, you guys, I'm going for anything here, OK?\", 'I cannot be a waitress anymore, I mean it.', \"I'm sick of the lousy tips, I'm sick of being called Excuse me.\", 'Ross: Rach, did you proofread these?', 'Rachel: Uh... yeah, why?', \"Ross: Uh, nothing, I'm sure they'll be impressed with your excellent compuper skills.\", 'Rachel: (upset) Oh my Goood!', \"Oh, do you think it's on all of them?\", \"Joey: Oh no, I'm sure the Xerox machine caught a few.\"]\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(script)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ross Uh Rach were running low on resumes over here', 'Monica Do you really want a job with Popular Mechanics', 'Chandler Well if youre gonna work for mechanics those are the ones to work for', 'Rachel Hey look you guys Im going for anything here OK', 'I cannot be a waitress anymore I mean it', 'Im sick of the lousy tips Im sick of being called Excuse me', 'Ross Rach did you proofread these', 'Rachel Uh yeah why', 'Ross Uh nothing Im sure theyll be impressed with your excellent compuper skills', 'Rachel upset Oh my Goood', 'Oh do you think its on all of them', 'Joey Oh no Im sure the Xerox machine caught a few']\n"
     ]
    }
   ],
   "source": [
    "def remove_delimiters(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "trimmed_sentences = [remove_delimiters(sent) for sent in sentences]\n",
    "print(trimmed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ross', 'Uh', 'Rach', 'were', 'running', 'low', 'on', 'resumes', 'over', 'here'], ['Monica', 'Do', 'you', 'really', 'want', 'a', 'job', 'with', 'Popular', 'Mechanics'], ['Chandler', 'Well', 'if', 'youre', 'gon', 'na', 'work', 'for', 'mechanics', 'those', 'are', 'the', 'ones', 'to', 'work', 'for'], ['Rachel', 'Hey', 'look', 'you', 'guys', 'Im', 'going', 'for', 'anything', 'here', 'OK'], ['I', 'can', 'not', 'be', 'a', 'waitress', 'anymore', 'I', 'mean', 'it'], ['Im', 'sick', 'of', 'the', 'lousy', 'tips', 'Im', 'sick', 'of', 'being', 'called', 'Excuse', 'me'], ['Ross', 'Rach', 'did', 'you', 'proofread', 'these'], ['Rachel', 'Uh', 'yeah', 'why'], ['Ross', 'Uh', 'nothing', 'Im', 'sure', 'theyll', 'be', 'impressed', 'with', 'your', 'excellent', 'compuper', 'skills'], ['Rachel', 'upset', 'Oh', 'my', 'Goood'], ['Oh', 'do', 'you', 'think', 'its', 'on', 'all', 'of', 'them'], ['Joey', 'Oh', 'no', 'Im', 'sure', 'the', 'Xerox', 'machine', 'caught', 'a', 'few']]\n"
     ]
    }
   ],
   "source": [
    "all_words = [nltk.word_tokenize(sent) for sent in trimmed_sentences]\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ross', 'Uh', 'Rach', 'we', 're', 'running', 'low', 'on', 'resumes', 'over', 'here', 'Monica', 'Do', 'you', 'really', 'want', 'a', 'job', 'with', 'Popular', 'Mechanics', 'Chandler', 'Well', 'if', 'you', 're', 'gonna', 'work', 'for', 'mechanics', 'those', 'are', 'the', 'ones', 'to', 'work', 'for', 'Rachel', 'Hey', 'look', 'you', 'guys', 'I', 'm', 'going', 'for', 'anything', 'here', 'OK', 'I', 'cannot', 'be', 'a', 'waitress', 'anymore', 'I', 'mean', 'it', 'I', 'm', 'sick', 'of', 'the', 'lousy', 'tips', 'I', 'm', 'sick', 'of', 'being', 'called', 'Excuse', 'me', 'Ross', 'Rach', 'did', 'you', 'proofread', 'these', 'Rachel', 'Uh', 'yeah', 'why', 'Ross', 'Uh', 'nothing', 'I', 'm', 'sure', 'they', 'll', 'be', 'impressed', 'with', 'your', 'excellent', 'compuper', 'skills', 'Rachel', 'upset', 'Oh', 'my', 'Goood', 'Oh', 'do', 'you', 'think', 'it', 's', 'on', 'all', 'of', 'them', 'Joey', 'Oh', 'no', 'I', 'm', 'sure', 'the', 'Xerox', 'machine', 'caught', 'a', 'few']\n"
     ]
    }
   ],
   "source": [
    "all_words_easy = re.findall(r'\\w+', script)\n",
    "print(all_words_easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'into', 'such', \"aren't\", 'most', 'm', 'you', 'not', 'but', 'me', 'that', 'did', 'about', 'hasn', 'by', 'each', 'until', 'when', \"mustn't\", 'ours', 'theirs', 'should', 'were', \"shan't\", 'so', 'doesn', \"won't\", 'own', 'same', 'after', 'because', 'out', 'if', \"you've\", 'being', 'more', 'has', 'hadn', \"wasn't\", \"you'll\", 'yours', 'the', 'above', 'all', 'how', 'themselves', 'a', 'himself', 'while', 'have', 'her', 'between', 'd', 'can', \"haven't\", 'our', 'which', 'had', \"she's\", 'off', 's', 'before', 'them', 'it', 'at', 'needn', 'shouldn', 'this', \"wouldn't\", \"couldn't\", 'and', 'doing', 'mustn', \"hasn't\", 'there', \"should've\", 'no', 'why', \"that'll\", 'ma', 'ourselves', 'wasn', 'itself', 'very', \"you're\", 'in', 'myself', 'for', 'y', 'won', 'does', \"it's\", 'they', 'further', 'now', 'his', 'haven', 'over', 'on', 'll', 'isn', 'aren', 're', 'these', 'o', 'what', \"doesn't\", 'she', \"didn't\", 'couldn', 'up', 'once', 'to', 'just', 'those', 'than', 'was', 'then', \"shouldn't\", 'your', 'down', 'against', 'again', \"don't\", 'below', 'shan', 'be', 'any', 'are', 'herself', 'been', 'from', \"weren't\", 'nor', 'didn', 'whom', 'other', 'or', 'of', 'having', 'where', 'an', 'both', 'him', 'few', 't', 'who', 'my', 'he', 'i', 'under', 'ain', 'wouldn', 'weren', 'yourself', 'hers', 've', \"you'd\", 'through', 'yourselves', 'is', 'with', 'do', 'will', 'as', \"hadn't\", 'am', 'some', 'during', 'too', 'here', 'don', 'only', \"needn't\", \"mightn't\", 'we', \"isn't\", 'mightn', 'their', 'its'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ross', 'Uh', 'Rach', 'running', 'low', 'resumes', 'Monica', 'Do', 'really', 'want', 'job', 'Popular', 'Mechanics', 'Chandler', 'Well', 'gonna', 'work', 'mechanics', 'ones', 'work', 'Rachel', 'Hey', 'look', 'guys', 'I', 'going', 'anything', 'OK', 'I', 'cannot', 'waitress', 'anymore', 'I', 'mean', 'I', 'sick', 'lousy', 'tips', 'I', 'sick', 'called', 'Excuse', 'Ross', 'Rach', 'proofread', 'Rachel', 'Uh', 'yeah', 'Ross', 'Uh', 'nothing', 'I', 'sure', 'impressed', 'excellent', 'compuper', 'skills', 'Rachel', 'upset', 'Oh', 'Goood', 'Oh', 'think', 'Joey', 'Oh', 'I', 'sure', 'Xerox', 'machine', 'caught']\n"
     ]
    }
   ],
   "source": [
    "words_without_stopwords = [word for word in all_words_easy if word not in stop_words]\n",
    "print(words_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('You', 'PRP'), ('are', 'VBP'), ('awesome', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "string = 'You are awesome'\n",
    "words = nltk.word_tokenize(string)\n",
    "pos_tag = nltk.pos_tag(words)\n",
    "print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook\n",
      "serv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "print(ps.stem('cooking'))\n",
    "print(ps.stem('serving'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook\n",
      "serve\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "print(lm.lemmatize('cooking', 'v'))\n",
    "print(lm.lemmatize('serving', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7f319784a9c046e08a711882be2618c3-0\" class=\"displacy\" width=\"575\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">You</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">awesome</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f319784a9c046e08a711882be2618c3-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f319784a9c046e08a711882be2618c3-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f319784a9c046e08a711882be2618c3-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f319784a9c046e08a711882be2618c3-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,91.5 L408.0,79.5 392.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy #another open-source library for NLP\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "from spacy import displacy\n",
    "temp = nlp('You are awesome')\n",
    "displacy.render(temp, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (PERSON Chandler/NNP) works/VBZ at/IN (ORGANIZATION Google/NNP))\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Chandler works at Google\"\n",
    "print (nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
